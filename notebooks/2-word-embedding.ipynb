{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial // Exploring Gender Bias in Word Embedding\n",
    "\n",
    "## https://learn.responsibly.ai/word-embedding\n",
    "\n",
    "Powered by [`responsibly`](https://docs.responsibly.ai/) - Toolkit for auditing and mitigating bias and fairness of machine learning systems üîéü§ñüß∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Three: Motivation - Why to use Word Embeddings?\n",
    "\n",
    "## 3.1 - [NLP (Natural Language Processing)](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "**Very partial** list of tasks\n",
    "\n",
    "\n",
    "### 1. Classification\n",
    "- Fake news classification\n",
    "- Toxic comment classification\n",
    "- Review raiting (sentiment analysis)\n",
    "- Hiring decision making by CV\n",
    "- Automated essay scoring\n",
    "\n",
    "### 3. Machine Translation\n",
    "\n",
    "### 2. Information Retrieval\n",
    "- Search engine\n",
    "- Plagiarism detection\n",
    "\n",
    "### 3. Conversation chatbot\n",
    "\n",
    "### 4. Coreference Resolution\n",
    "![](../images/corefexample.png)\n",
    "<small>Source: [Stanford Natural Language Processing Group](https://nlp.stanford.edu/projects/coref.shtml)</small>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "## 3.2 - Machine Learning (NLP) Pipeline\n",
    "<br>\n",
    "<div style=\"border: 1px solid; padding: 50px; margin: 10px\">\n",
    " <h2>\n",
    " \n",
    "Data ‚Üí Representation ‚Üí (Structured) Inference ‚Üí Prediction   \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üë\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Auxiliary Corpus/Model\n",
    " </h2>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<small>Source: [Kai-Wei Chang (UCLA) - What It Takes to Control Societal Bias in Natural Language Processing](https://www.youtube.com/watch?v=RgcXD_1Cu18)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Esessional Question - How to represent language to machine?\n",
    "\n",
    "We need some kind of *dictionary* üìñ to transform/encode\n",
    "\n",
    "... from a human representation (words) üó£ üî°\n",
    "\n",
    "... to a machine representation (numbers) ü§ñ üî¢\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "## First Atempt\n",
    "\n",
    "### Idea: Bag of Words (for a document)\n",
    "![](../images/bow.png)\n",
    "<small>Source: Zheng, A.& Casari, A. (2018). Feature Engineering for Machine Learning. O'Reilly Media.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocabulary = ['it', 'they', 'puppy', 'and', 'cat', 'aardvark', 'cute', 'extremely', 'not']\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'it is a puppy and it is extremely cute'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform([sentence]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(['it is not a puppy and it is extremely cute']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(['it is a puppy and it is extremely not cute']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü¶Ñ Read more about scikit-learn's text feature extraction [here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[vectorizer.fit_transform([word]).toarray()\n",
    " for word in sentence.split()\n",
    " if word in vocabulary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem with one-hot representation\n",
    "\n",
    "![](../images/audio-image-text.png)\n",
    "<small>Source: [Tensorflow Documentation](https://www.tensorflow.org/tutorials/representation/word2vec)</small>\n",
    "\n",
    "[Color Picker](https://www.google.com/search?q=color+picker)\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "## 3.4 - üíé Idea: Embedding a word in a n-dimensional space\n",
    "\n",
    "### Distributional Hypothesis\n",
    "> \"a word is characterized by the company it keeps\" - [John Rupert Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth)\n",
    "\n",
    "**Distance ~ Meaning Similarity**\n",
    "\n",
    "\n",
    "### ü¶Ñ Examples (algorithms and pre-trained models)\n",
    "- [Word2Vec](https://code.google.com/archive/p/word2vec/)\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [fastText](https://fasttext.cc/)\n",
    "- [ELMo](https://allennlp.org/elmo) (contextualized)\n",
    "\n",
    "#### ü¶Ñ Training: using *word-context* relationships from a corpus. See: [The Illustrated Word2vec by Jay Alammar](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "#### ü¶Ñ State of the Art - Contextual Word Embedding ‚Üí Language Models\n",
    "- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) by Jay Alammar](http://jalammar.github.io/illustrated-bert/)\n",
    "- Microsoft - [NLP Best Practices](https://github.com/microsoft/nlp-recipes)\n",
    "- [Tracking Progress in Natural Language Processing](https://nlpprogress.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Four: Playing with Word2Vec word embedding!\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) - Google News - 100B tokens, 3M vocab, cased, 300d vectors - only lowercase vocab extracted\n",
    "\n",
    "Loaded using [`responsibly`](http://docs.responsibly.ai) package, the function [`responsibly.we.load_w2v_small`]() returns a [`gensim`](https://radimrehurek.com/gensim/)'s [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors) object.\n",
    "\n",
    "\n",
    "## 4.1 - Basic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è‚ö° ignore warnings\n",
    "# generally, you shouldn't do that, but for this tutorial we'll do so for the sake of simplicity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import load_w2v_small\n",
    "\n",
    "w2v_small = load_w2v_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "\n",
    "len(w2v_small.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector of the word \"home\"\n",
    "\n",
    "print('home =', w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the word embedding dimension, in this case, is 300\n",
    "\n",
    "len(w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the words are normalized (=have norm equal to one as vectors)\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "norm(w2v_small['home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è make sure that all the vectors are normalized!\n",
    "\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "length_vectors = norm(w2v_small.vectors, axis=1)\n",
    "\n",
    "assert_almost_equal(actual=length_vectors,\n",
    "                    desired=1,\n",
    "                    decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - üíé Mesuring Distance between Words\n",
    "\n",
    "![](../images/sphere.png)\n",
    "\n",
    "<small>Source: [Wikipedia](https://en.wikipedia.org/wiki/File:Sphere_wireframe_10deg_6r.svg)</small>\n",
    "\n",
    "### Mesure of Similiarty: [Cosine Similariy](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "- Measures the cosine of the angle between two vecotrs.\n",
    "- Ranges between 1 (same vector) to -1 (opposite/antipode vector)\n",
    "- In Python, for normalized vectors (Numpy's array), use the `@`(at) operator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import acos, degrees\n",
    "\n",
    "degrees(acos(w2v_small['cat'] @ w2v_small['cats']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees(acos(w2v_small['cat'] @ w2v_small['dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['cow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees(acos(w2v_small['cat'] @ w2v_small['cow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small['cat'] @ w2v_small['graduated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees(acos(w2v_small['cat'] @ w2v_small['graduated']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíé In general, the use of Word Embedding to encode words, as an input for NLP systems (*), improve their performance compared to one-hot representation.\n",
    "\n",
    "\\* Sometimes the embedding is learned as part of the NLP system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - üõ†Ô∏è Visualization Word Embedding in 2D using T-SNE \n",
    "\n",
    "<small>Source: [Google's Seedbank](https://research.google.com/seedbank/seed/pretrained_word_embeddings)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "# take the most common words in the corpus between 200 and 600\n",
    "words = [word for word in w2v_small.index2word[200:600]]\n",
    "\n",
    "# convert the words to vectors\n",
    "embeddings = [w2v_small[word] for word in words]\n",
    "\n",
    "# perform T-SNE\n",
    "words_embedded = TSNE(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "# ... and visualize!\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, label in enumerate(words):\n",
    "    x, y = words_embedded[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                 ha='right', va='bottom', size=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: [Tensorflow Embedding Projector](http://projector.tensorflow.org)\n",
    "‚ö° Be cautious: It is easy to see \"patterns\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Most Similar\n",
    "\n",
    "What are the most simlar words (=closer) to a given word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA: Doesn't Match\n",
    "\n",
    "Given a list of words, which one doesn't match?\n",
    "\n",
    "The word further away from the mean of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 - Vector Arithmetic\n",
    "\n",
    "![](../images/vector-addition.png)\n",
    "\n",
    "<small>Source: [Wikipedia](https://commons.wikimedia.org/wiki/File:Vector_add_scale.svg)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nature + science = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['nature', 'science'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 - üíé Vector Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/linear-relationships.png)\n",
    "<small>Source: [Tensorflow Documentation](https://www.tensorflow.org/tutorials/representation/word2vec)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# man:king :: woman:?\n",
    "# king - man + woman = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['king', 'woman'],\n",
    "                       negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['big', 'smaller'],\n",
    "                       negative=['small'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 - Think about a DIRECTION in word embedding as a RELATION\n",
    "\n",
    "# $\\overrightarrow{she} - \\overrightarrow{he}$\n",
    "# $\\overrightarrow{smaller} - \\overrightarrow{small}$\n",
    "# $\\overrightarrow{Spain} - \\overrightarrow{Madrid}$\n",
    "\n",
    "\n",
    "**‚ö° Direction is not a word vector by itself!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° But it doesn't work all the time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['forward', 'up'],\n",
    "                       negative=['down'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be because we have the phrase \"looking forward\" which is acossiated with \"excitement\" in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö°ü¶Ñ Keep in mind the word embedding was generated by learning the co-occurrence of words, so the fact that it *empirically* exhibit \"concept arithmetic\", it doesn't necessarily mean it learned it! In fact, it seems it didn't.\n",
    "See: [king - man + woman is queen; but why? by Piotr Migda≈Ç](https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)\n",
    "\n",
    "ü¶Ñ EXTRA: [Demo - Word Analogies Visualizer by Julia Bazi≈Ñska](https://lamyiowce.github.io/word2viz/)\n",
    "\n",
    "‚ö°ü¶Ñ In fact, `w2v_small.most_similar` find the most closest word which *is not one* of the given ones. This is a real methodological issue. Nowadays, it is not a common practice to evaluate word embedding with analogies.\n",
    "\n",
    "You can use [`responsibly.we.most_similar`](https://docs.responsibly.ai/word-embedding-bias.html#responsibly.we.utils.most_similar) for the unrestricted version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
