{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial // Exploring Gender Bias in Word Embedding\n",
    "\n",
    "## https://learn.responsibly.ai/word-embedding\n",
    "\n",
    "Powerd by [`responsibly`](https://docs.responsibly.ai/) - Toolkit for auditing and mitigating bias and fairness of machine learning systems üîéü§ñüß∞\n",
    "\n",
    "# Overview\n",
    "\n",
    "## Learning Objective:\n",
    "\n",
    "1. Gain intuitive technical understanding of bias in machine learning systems.\n",
    "\n",
    "2. Explore the interplay between data, algorithms, application, workflow and human context.\n",
    "\n",
    "## Audiance\n",
    "Everyone, really. No previous knowledge is assumed. If you do have background, then you will be able to see the topic in a deeper way throguh thw workshop.\n",
    "\n",
    "\n",
    "## Mothod\n",
    "\n",
    "1. Dive into one family of machine learning models/building-block as a scaffolder.\n",
    "\n",
    "2. Focus on one example of bias: by gender.\n",
    "\n",
    "## Disclaimers\n",
    "\n",
    "1. Word embeddings are not very important by themselves in the context of responsible AI, but bias can be demonstrated with them in an intuitiive way.\n",
    "\n",
    "2. We focus on gender bias, and treate it as binary for the simplicity in this work. Neverthless, gender is a complex soical construct and we should keep it in mind when we go back from learning context to the real-world.\n",
    "\n",
    "2. We don't aim to give a comprehensive overview of neither bias in machine learning nor fairness nor responsible AI.\n",
    "\n",
    "3. If you need to work on one of these topics, this workshop is far from being enough, but it can serve as a starting point for your learning.\n",
    "\n",
    "4. On top of that, it is an active area of research.\n",
    "\n",
    "5. And there is much more to say about this topic, especially from Science and technology studies (STS) point of view (but not only).\n",
    "\n",
    "5. Therfore we will supply good learning resources at end.\n",
    "\n",
    "\n",
    "## Legend\n",
    "üíé Important\n",
    "\n",
    "‚ö° Be Aware - Debated issue / interpret carefully / simplicity over precision\n",
    "\n",
    "üõ†Ô∏è Setup/Technical (a.k.a \"the code is not important, just run it!\")\n",
    "\n",
    "üß™ Methodological Issue\n",
    "\n",
    "üíª Hands-On - Your turn! NO programming background\n",
    "\n",
    "‚å®Ô∏è ... Some programming background (in Python) is required\n",
    "\n",
    "ü¶Ñ Out of Scope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - üõ†Ô∏è Install `responsibly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user git+https://github.com/ResponsiblyAI/responsibly.git@dev\n",
    "# !pip install --user responsibly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - üõ†Ô∏è Validate Installation of `responsibly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import responsibly\n",
    "\n",
    "# You should get '0.1.2'\n",
    "responsibly.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ö†Ô∏è You might need to restart youe notebook\n",
    "\n",
    "If you get an error of **`ModuleNotFoundError: No module named 'responsibly'`** after the installation, and you work on either **Colab** or **Binder** - this is **normal**.\n",
    "<br/> <br/>\n",
    "**Restart** the Kernel/Runtime (use the menu on top or the botton in the notebook), **skip** the installation cell (`!pip install --user responsibly`) and **run** the previous cell again (`import responsibly`).\n",
    "\n",
    "Now it should all work fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two: Examples of Bias in Language Technology\n",
    "\n",
    "## 2.1 - Translation\n",
    "\n",
    "![](../images/example-translate.jpg)\n",
    "\n",
    "Source: [Google Blog](https://www.blog.google/products/translate/reducing-gender-bias-google-translate/), [Google AI Blog](https://ai.googleblog.com/2018/12/providing-gender-specific-translations.html)\n",
    "\n",
    "## 2.2 - Automated Speech Recognition (ASR) \n",
    "\n",
    "![](../images/asr-wer.jpg)\n",
    "\n",
    "WER = Average Word Error Rate\n",
    "\n",
    "`(substitutions + deletions + insertions) / total number of words`\n",
    "\n",
    "Koenecke, Allison, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor Toups, John R. Rickford, Dan Jurafsky, and Sharad Goel. \"[Racial disparities in automated speech recognition](https://www.pnas.org/content/117/14/7684).\" Proceedings of the National Academy of Sciences 117, no. 14 (2020): 7684-7689.\n",
    "\n",
    "[Stanford News](https://news.stanford.edu/2020/03/23/automated-speech-recognition-less-accurate-blacks/)\n",
    "\n",
    "## 2.3 - Recruiting tool\n",
    "\n",
    "\"Amazon scraps secret AI recruiting tool that showed bias against women\" ([Reuters](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G))\n",
    "\n",
    "\"But by 2015, the company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way.\"\n",
    "\n",
    "![](../images/banner.png)\n",
    "\n",
    "## 2.4 - Natural Language Generation  (based on language models)\n",
    "\n",
    "[Write With Transformer](https://transformer.huggingface.co/doc/gpt2-large) (OpenAI GPT-2)\n",
    "\n",
    "![](../images/nlg-prompt.png)\n",
    "\n",
    "1. Sheng, E., Chang, K. W., Natarajan, P., & Peng, N. (2019). [The woman worked as a babysitter: On biases in language generation](https://arxiv.org/pdf/1909.01326.pdf). arXiv preprint arXiv:1909.01326.\n",
    "2. [StereoSet](https://stereoset.mit.edu/)\n",
    "Nadeem, M., Bethke, A., & Reddy, S. (2020). [StereoSet: Measuring stereotypical bias in pretrained language models](https://arxiv.org/pdf/2004.09456.pdf). arXiv preprint arXiv:2004.09456."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
