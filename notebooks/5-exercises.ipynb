{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqMESMMaTM59"
   },
   "source": [
    "# Workshop // Exploring Gender Bias in Word Embedding\n",
    "\n",
    "## https://learn.responsibly.ai/word-embedding\n",
    "\n",
    "Powerd by [`responsibly`](https://docs.responsibly.ai/) - Toolkit for auditing and mitigating bias and fairness of machine learning systems üîéü§ñüß∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVMLVzKaTM5-"
   },
   "source": [
    "# Part Eleven: Your Turn!\n",
    "<big>‚å®Ô∏è</big>\n",
    "\n",
    "Note: The first two tasks require a basic background in Python programming. For the last task, you need some experience with Machine Learning and Natural Langauge Processing (NLP) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgsASXY2TM5_"
   },
   "outputs": [],
   "source": [
    "from responsibly.we import load_w2v_small\n",
    "\n",
    "w2v_small = load_w2v_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPVjrwpwTM6G"
   },
   "source": [
    "## Task 1 - Racial bias\n",
    "\n",
    "Let's explor racial bias usint Tolga's approche. Will use the [`responsibly.we.BiasWordEmbedding`](http://docs.responsibly.ai/word-embedding-bias.html#ethically.we.bias.BiasWordEmbedding) class. `GenderBiasWE` is a sub-class of `BiasWordEmbedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gB82y7hTM6H"
   },
   "outputs": [],
   "source": [
    "from responsibly.we import BiasWordEmbedding\n",
    "\n",
    "w2v_small_racial_bias = BiasWordEmbedding(w2v_small, only_lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5Pa-_OOTM6J"
   },
   "source": [
    "üíéüíéüíé Identify the racial direction using the `sum` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8NjJ3fPTM6K"
   },
   "outputs": [],
   "source": [
    "white_common_names = ['Emily', 'Anne', 'Jill', 'Allison', 'Laurie', 'Sarah', 'Meredith', 'Carrie',\n",
    "                      'Kristen', 'Todd', 'Neil', 'Geoffrey', 'Brett', 'Brendan', 'Greg', 'Matthew',\n",
    "                      'Jay', 'Brad']\n",
    "\n",
    "black_common_names = ['Aisha', 'Keisha', 'Tamika', 'Lakisha', 'Tanisha', 'Latoya', 'Kenya', 'Latonya',\n",
    "                      'Ebony', 'Rasheed', 'Tremayne', 'Kareem', 'Darnell', 'Tyrone', 'Hakim', 'Jamal',\n",
    "                      'Leroy', 'Jermaine']\n",
    "\n",
    "w2v_small_racial_bias._identify_direction('Whites', 'Blacks',\n",
    "                                          definitional=(white_common_names, black_common_names),\n",
    "                                          method='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQzNVMgITM6N"
   },
   "source": [
    "Use the neutral profession names to measure the racial bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VsIaNeTTM6N"
   },
   "outputs": [],
   "source": [
    "from responsibly.we.data import BOLUKBASI_DATA\n",
    "\n",
    "neutral_profession_names = BOLUKBASI_DATA['gender']['neutral_profession_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "iQMUSBpKTM6Q",
    "outputId": "d129a64f-84a4-455d-f874-782788f376a4"
   },
   "outputs": [],
   "source": [
    "neutral_profession_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "E3appKv5TM6S",
    "outputId": "95025f45-35ce-4d7b-b399-c97a8c2e3643"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_racial_bias.plot_projection_scores(neutral_profession_names, n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvmZXAdKTM6Z"
   },
   "source": [
    "Calculate the direct bias measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80S8S-3hTM6a"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSFdc0NWTM6d"
   },
   "source": [
    "Keep exploring the racial bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDjRDxz_TM6d"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9mOVcXWTM6g"
   },
   "source": [
    "## Task 2 - Your WEAT test\n",
    "\n",
    "Open the [word embedding demo page in `responsibly` documentation](http://docs.responsibly.ai/notebooks/demo-word-embedding-bias.html#it-is-possible-also-to-expirements-with-new-target-word-sets-as-in-this-example-citizen-immigrant), and look on the use of the function `calc_weat_pleasant_unpleasant_attribute`. What was the attempt in that experiment? What was the result? Can you come up with other experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvlhI95-TM6h"
   },
   "outputs": [],
   "source": [
    "from responsibly.we import calc_weat_pleasant_unpleasant_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMmc9vNwTM6j"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fud0_MaTM6o"
   },
   "source": [
    "## Task 3 - Sentiment Analysis\n",
    "\n",
    "Nots: This task requires some background with NLP, particularly having an experience with training a text classifier in Python.\n",
    "\n",
    "One way to examine bias in word embeddings is through a downstream application. Here we will use a sentiment analysis classifier of tweets; given a tweet, the system would infer the *valence* of the sentiment expressed in a tweet. The valence is expressed as a real number between 0 and 1, where 0 represents the negative and 1 is for the positive end.\n",
    "\n",
    "The system is going to be rather simple and consists of three components:\n",
    "\n",
    "1. Preprocessing (e.g., removing stopwords and punctuation, [tockenization](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation))\n",
    "2. Transforming the tweets' tokens  into a single 300-dimensional vector.\n",
    "3. Applying logistic regression to predict the valence.\n",
    "\n",
    "Our goal is to assess the word embedding's impact in its original version and the neutralize-\"debiased\" one on the system bias. We are going to build two versions of that system, each using one version of the two word embedding, and compare its performance on the [Equity Evaluation Corpus (EEC)](http://saifmohammad.com/WebPages/Biases-SA.html), which is designed to assess gender bias in sentiment analysis systems.\n",
    "\n",
    "**Reference:**\n",
    "Kiritchenko, S., & Mohammad, S. M. (2018). [Examining gender and race bias in two hundred sentiment analysis systems](https://arxiv.org/pdf/1805.04508.pdf). arXiv preprint arXiv:1805.04508."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZY369yFKD91"
   },
   "source": [
    "### Data\n",
    "\n",
    "First, let's load the datasets \"Affect in Tweets\" taken from the [SemEval 2018](https://competitions.codalab.org/competitions/17751#learn_the_details-datasets) competition. We have training, development, and test datasets. We will only use the first and the last datasets, but feel free to use the development dataset to tune select models and hyperparameters with cross-validation.\n",
    "\n",
    "There are three columns:\n",
    "\n",
    "1. `Tweet` - The tweet itself as a string, the input.\n",
    "2. `Intensity Score` - The sentiment's valence of the tweet in the range [0, 1], the output\n",
    "3. `Affect Dimension` - You can ignore it. It is `'valence'` for all of the data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w6q4eEPKCu4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('../data/SemEval2018-Task1-all-data/English/V-reg/2018-Valence-reg-En-train.txt',\n",
    "                       sep='\\t', index_col=0)\n",
    "dev_df = pd.read_csv('../data/SemEval2018-Task1-all-data/English/V-reg/2018-Valence-reg-En-dev.txt',\n",
    "                       sep='\\t', index_col=0)\n",
    "test_df = pd.read_csv('../data/SemEval2018-Task1-all-data/English/V-reg/2018-Valence-reg-En-test-gold.txt',\n",
    "                       sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrtR0AaLKC1f"
   },
   "outputs": [],
   "source": [
    "# A few examples\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuw2NxOJKCyR"
   },
   "outputs": [],
   "source": [
    "# Convert all the labels from real numbers into boolean values,\n",
    "# setting the threshold at 0.5, and creating a new column named\n",
    "# `label`\n",
    "\n",
    "train_df['label'] = train_df['Intensity Score'] > 0.5\n",
    "dev_df['label'] = dev_df['Intensity Score'] > 0.5\n",
    "test_df['label'] = test_df['Intensity Score'] > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8ktamj9Lzvw"
   },
   "source": [
    "Now, let's download the **complete** word2voc word embedding, (which is not filtered only to lowercased words), and load it using the `gensim` Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJLEhGpaLyH2"
   },
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjmXQTC9Lw3U"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Limit vocabulary to top-500K most frequent words\n",
    "VOCAB_SIZE = 500000\n",
    "\n",
    "# Load the word2vec\n",
    "w2v_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',\n",
    "                                              binary=True,\n",
    "                                              limit=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mv2O-OS6KCsq"
   },
   "outputs": [],
   "source": [
    "# Get the vector embedding for a word\n",
    "w2v_model['home']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-eIToHETM6o"
   },
   "outputs": [],
   "source": [
    "# Check whether there is an embedding for a word\n",
    "'bazinga' in w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8LfyJnzNIy1"
   },
   "source": [
    "### Preprocessing & feature extraction\n",
    "\n",
    "Before we transform a tweet into a vector of 300 dimensions, it should be broken into tokens (\"words\") and be cleaned. You can do that with various Python packages for NLP, such as [NLTK](https://www.nltk.org/) and \n",
    "[spaCy](https://spacy.io/). Feel free to use them if you would like to! We will use the basic preprocessing functionality that comes with [`gensim`](https://radimrehurek.com/gensim/parsing/preprocessing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbQRZlFIshUb"
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import (preprocess_string,\n",
    "                                          strip_tags,\n",
    "                                          strip_punctuation,\n",
    "                                          strip_multiple_whitespaces,\n",
    "                                          strip_numeric,\n",
    "                                          remove_stopwords)\n",
    "\n",
    "\n",
    "# We pick a subset of the default filters,\n",
    "# in particular, we do not take\n",
    "# strip_short() and stem_text().\n",
    "FILTERS = [strip_punctuation,\n",
    "           strip_tags,\n",
    "           strip_multiple_whitespaces,\n",
    "           strip_numeric,\n",
    "           remove_stopwords]\n",
    "\n",
    "# See how the sentece is transformed into tokes (words)\n",
    "preprocess_string('This is a \"short\" text!', FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_pi5WFOOcsb"
   },
   "source": [
    "After preprocessing all the tweets, we get tokens. We transform each token into a 300d vector using the word embedding and then compute the *average* vector. It will have 300 dimensions as well. This vector serves as the values of the features for each tweet. \n",
    "\n",
    "Note for these two possible pitfalls:\n",
    "\n",
    "1. Make sure that the token exists int he word embedding.\n",
    "2. Sometimes, there are tweets without any token found in the word embedding. Discard these tweets from the data. Keep in mind that you should discard the labels as well.\n",
    "\n",
    "Write the function `generate_text_features(text, w2v)` that gets a string `text` and a word embedding `w2v` and produces the features of this text according to the method xdescribed above. The function should return an Numpy array with lengh of 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEn8ecmrsutk"
   },
   "outputs": [],
   "source": [
    "def generate_text_features(text, w2v):\n",
    "    pass  # Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDSaVGfzP0Em"
   },
   "source": [
    "Now, use this function to produce the features for all three datasets (training, validation, test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sl0aYxLNQTs3"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEM-riKyvdKo"
   },
   "source": [
    "### Training a classifier\n",
    "\n",
    "The next step is straightforward, train logistic regression on the dataset. Report the accuracy of the training and the test dataset.\n",
    "\n",
    "We recommend using [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eMRIjj3REvG"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdDLoHIBRV_B"
   },
   "source": [
    "### Evaluate gender bias in the downstream appliation\n",
    "\n",
    "The **Equity Evaluation Corpus (EEC)** consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders.\n",
    "\n",
    "We foucs on the sentences releated to gender. Every sentence is a build out of three elements:\n",
    "\n",
    "1. Person (e.g., `he`, `this woman`, `my uncly`, `my mother`)\n",
    "2. Emotion word (e.g., `anger`, `happy`, `gloomy`, `amazing`)\n",
    "3. Template (e.g., `<person subject> feels <emotion word>`).\n",
    "\n",
    "that are mixed together to form a sentence, for examples:\n",
    "* he feels anger\n",
    "* she feels anger\n",
    "* this woman feels happy\n",
    "* this man feels happy\n",
    "\n",
    "Thanks to this systemic constraction from templates, the sentence are paired by gender, i.e. the EEC data is built of pairs of sentences that are all the same except for a gender noun (e.g., `she`-`he`, `my mother`-`my father`). If we think about sentiment analysis, there is no reason that the a system would assign different prediction to the paird sentences! So if we find differce in that, it could point for a potential gender bias in the downstream application.\n",
    "\n",
    "#### Keep in mind, this is only an operalization of the gender bias in a sentiment analysis system. All the issues with a concreate and single measurement arise also here! We should always take into accout the human contex in which the system is deployed!\n",
    "\n",
    "\n",
    "The following cell is just for some data preperation, and it is not important to understand it; neverthless, make sure you run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Z60x-8iuWGM"
   },
   "outputs": [],
   "source": [
    "# üõ† Prepare the EEC data, no need to dig into this cell\n",
    "\n",
    "eec_df = pd.read_csv('../data/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv')\n",
    "\n",
    "# Remove the sentences for evaluating racial bias\n",
    "gender_eec_df = eec_df[eec_df['Race'].isna()][:]\n",
    "\n",
    "# Create identifier to mach sentence pairs\n",
    "# The EEC data comes withot this matching\n",
    "MALE_PERSONS = ('he', 'this man', 'this boy', 'my brother', 'my son', 'my husband',\n",
    "                'my boyfriend', 'my father', 'my uncle', 'my dad', 'him')\n",
    "\n",
    "FEMALE_PERSONS = ('she', 'this woman', 'this girl', 'my sister', 'my daughter', 'my wife',\n",
    "                  'my girlfriend', 'my mother', 'my aunt', 'my mom', 'her')\n",
    "\n",
    "MALE_IDENTIFIER = dict(zip(MALE_PERSONS, FEMALE_PERSONS))\n",
    "FEMALE_IDENTIFIER = dict(zip(FEMALE_PERSONS, FEMALE_PERSONS))\n",
    "\n",
    "PERSON_MATCH_WORDS = {**MALE_IDENTIFIER,\n",
    "                      **FEMALE_IDENTIFIER}\n",
    "\n",
    "gender_eec_df['PersonIdentifier'] = gender_eec_df['Person'].map(PERSON_MATCH_WORDS)\n",
    "\n",
    "gender_eec_df = gender_eec_df.sort_values(['Gender', 'Template', 'Emotion word', 'PersonIdentifier'])\n",
    "\n",
    "gender_split_index = len(gender_eec_df) // 2\n",
    "\n",
    "# Create two DataFrames, one for \n",
    "female_eec_df = gender_eec_df[:gender_split_index].reset_index(False)\n",
    "male_eec_df = gender_eec_df[gender_split_index:].reset_index(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yKi4SYCPcc4"
   },
   "outputs": [],
   "source": [
    "female_eec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwxLn-WbxbwV"
   },
   "outputs": [],
   "source": [
    "male_eec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KOoYeQCQ9JU"
   },
   "source": [
    "Note that the two DataFrames are paired by index. If we take that *i*-th row in each one of them, then they are different only in the matched person word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZCc8eadRH-k"
   },
   "outputs": [],
   "source": [
    "k = 543  # change my value and run the cell again!\n",
    "female_eec_df.iloc[k]['Sentence'], male_eec_df.iloc[k]['Sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TH3wHOe5xN4"
   },
   "source": [
    "Compute the probability estimations of the classifier for the female and male parts in the EEC data. If you use `sklearn`, then the classifier's method `predict_proba` is your friend for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASdKJWk_C9rq"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hGJspuayGmq"
   },
   "source": [
    "### Do the same for the neutralize-\"debiased\" word2vec\n",
    "\n",
    "Perform the all the previous steps for the neutralize-\"debiased\" word2vec to produce the probability estimations of the EEC data for the classifier using that word-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HAqdHCO7KhQ"
   },
   "source": [
    "#### Neutralize-\"debias\" the word embedding\n",
    "\n",
    "Hints:\n",
    "1. Use [`responsibly.we.GenderBiasWE`](https://docs.responsibly.ai/word-embedding-bias.html). \n",
    "2. Look for the method `debias`.\n",
    "3. Set the `method` argument to `'neutralize'`. \n",
    "4. Make sure that you set `inplace=True` to save memory. Note that you won't be able to work with the original word embedding after that.\n",
    "5. Validate the neutralize-\"debias\" was applied by computing the direct bias measure with the method `calc_direct_bias`.\n",
    "6. After the bias mitigating, the word embedding itself (as a `KeyedVectors` of `gensim`), is accessible through the attribute `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7CQ4Xz-7hBx"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuCEd3hVDNEF"
   },
   "source": [
    "#### Generate features with the \"debiased\" word embedding and train a new classifier\n",
    "\n",
    "Check the classifier's accuracy on the training and the test data - did the \"debiasing\" of the word embeddings hurt the classifier performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOe-fwnnDT7C"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PyZXIUiDU3l"
   },
   "source": [
    "#### Compute the probability estimations for the male and female sentences in the EEC data with the new classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTLXhU0RDjNy"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNldyVC_4O5k"
   },
   "source": [
    "### Gender bias analysis\n",
    "\n",
    "Now we are ready to blend all together. You have two classifiers, each one of them was trained on the same dataset, but with a different word embedding. The first used the original word2vec, and the other was undergone the neutralize-\"debias\" process. We computed the probability estimates for the EEC data twice for each one of the classifiers.\n",
    "\n",
    "\n",
    "**Think about how to evaluate the impact of replacing the word embedding concerning gender bias. Keep in mind that the female and male EEC data is paired!**\n",
    "\n",
    "#### Your analysis can take two points of view (there are more, but you start with that):\n",
    "1. Analyze the difference between the female and male probability estimations for each system *separately* and compare the results.\n",
    "2. Analyze the difference of differences; start with the difference of probability estimations between the paired female and male sentences for each system, and then compare the two differences.\n",
    "\n",
    "\n",
    "#### Few possible ideas of what to do:\n",
    "1. Plot distributions  ([`seaborn.displot`](https://seaborn.pydata.org/generated/seaborn.displot.html#seaborn.displot))\n",
    "2. Compute the [effect size](https://en.wikipedia.org/wiki/Effect_size#Cohen's_d)\n",
    "3. Perform statistical hypothesis testings to check whether means are eqaul using the paired t-test ([`scipy.stats.ttest_rel`])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz8SdPr9C-3p"
   },
   "outputs": [],
   "source": [
    "# Your Code Here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpwTNQr7Oe-a"
   },
   "source": [
    "#### What is your conclusion? What would be your next steps?\n",
    "\n",
    "Consider:\n",
    "\n",
    "1. Group by the analysis according to the EEC columns (e.g., by emotion)\n",
    "2. Try another classifier (e.g., `sklearn.ensemble.RandomForestClassifier`)\n",
    "3. Change the mitigation bias to *hard* instead of *neutralize*.\n",
    "4. Analyze the training data from gender prespective\n",
    "\n",
    "\n",
    "\n",
    "Refer to this paper for some ideas:\n",
    "[Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems](http://saifmohammad.com/WebDocs/EEC/ethics-StarSem-final_with_appendix.pdf). Svetlana Kiritchenko and Saif M. Mohammad. In Proceedings of *Sem, New Orleans, LA, USA, June 2018."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kopie von 5-exercises.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
